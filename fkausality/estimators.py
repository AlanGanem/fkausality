# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/estimators.ipynb (unless otherwise specified).

__all__ = ['pandas_gorupby_apply_parallel', 'idxs_split', 'SMFormulaWrapper', 'SMWrapper', 'CEDTEstimator',
           'pandas_gorupby_apply_parallel', 'idxs_split', 'SMFormulaWrapper', 'SMWrapper', 'CEDTEstimator']

# Cell
import multiprocessing
from multiprocessing import Pool, cpu_count
from functools import partial

import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator, RegressorMixin
import statsmodels.api as sm

from heartwood.kernel import JaccardForestKernel
from .utils import hstack, vstack
from .dist import sample_from_neighbors_continuous, estimate_mean_and_variance_from_neighbors_mixture

# Cell

def pandas_gorupby_apply_parallel(dfGrouped, func, **kwargs):
    with Pool(cpu_count()) as p:
        ret_list = p.map(partial(func, **kwargs), [group for name, group in dfGrouped])
    return ret_list

def idxs_split(a, n, axis = 0, drop_empty = True, return_slices = False):
    '''
    returns indices of the n splits of the a array along axis
    '''
    if not hasattr(a, 'shape'):
        a = np.arange(len(a))
    idxs = np.array_split(np.arange(a.shape[axis]), n, axis = 0)
    idxs = [list(i) for i in idxs]

    if drop_empty:
        idxs = [i for i in idxs if i]

    if return_slices:
        idxs = [slice(min(i), max(i)+1) for i in idxs if i]

    return idxs

def _agg_covariance(df, y_columns, T_columns, alpha):
    '''
    covariance of a given target sample in relation to some given Treatment
    '''

    import statsmodels.api as sm
    w = (1 - df['dissimilarity'])**alpha
    model = sm.WLS(df[y_columns],df[T_columns],w).fit()

    std = model.bse
    #std.columns = ['std' for _ in std.columns]
    slope = model.params
    #slope.columns = ['slope' for _ in slope.columns]
    count = (slope*0+1)*df.shape[0]
    pvalues = model.pvalues


    d = [slope,std,count, pvalues]

    idx = pd.Index(['slope','std','count','p_value'], name = 'statistic')
    d = pd.DataFrame(d, index = idx)


    return d

def _agg_mean_variance(df, y_columns, alpha, variance_mapper, min_var_factor):
    '''
    calculates mean and variance for a given sample in groupby
    '''
    mean, var = estimate_mean_and_variance_from_neighbors_mixture(
        df[y_columns].values,
        df['dissimilarity'].values,
        alpha = alpha,
        variance_mapper = variance_mapper,
        min_var_factor = min_var_factor,
    )

    count = df.shape[0]
    if var.ndim > 1:

        #std = var.diagonal()**(1/2)
        d = np.concatenate([mean,var,count*np.ones(mean.shape)]).reshape(3,len(y_columns))
    else:
        #std = var**1/2
        d = [mean,var,count]

    d = pd.DataFrame(d)


    d.index = pd.Index(['mean','variance','count'], name = 'statistic')
    d.columns = y_columns

    return d

# Cell
class SMFormulaWrapper(BaseEstimator, RegressorMixin):
    """
    A sklearn-style wrapper for formula-based statsmodels regressors,
    thanks to https://nelsonauner.com/data/2018/05/21/wrap-statsmodels-in-sklearn.html
    """
    def __init__(self, model_class, formula):
        self.model_class = model_class
        self.formula = formula
    def fit(self, X, y=None):
        self.model_ = self.model_class(self.formula, data=X)
        self.results_ = self.model_.fit()
    def predict(self, X):
        return self.results_.predict(X)

class SMWrapper(BaseEstimator, RegressorMixin):
    """
    A universal sklearn-style wrapper for statsmodels regressors ,
    thanks to https://stackoverflow.com/questions/41045752/using-statsmodel-estimations-with-scikit-learn-cross-validation-is-it-possible/
    """
    def __init__(self, model_class, fit_intercept=True):
        self.model_class = model_class
        self.fit_intercept = fit_intercept
    def fit(self, X, y):
        if self.fit_intercept:
            X = sm.add_constant(X)
        self.model_ = self.model_class(y, X)
        self.results_ = self.model_.fit()
        return self
    def predict(self, X):
        if self.fit_intercept:
            X = sm.add_constant(X)
        return self.results_.predict(X)

# Cell
class _FKEstimator(BaseEstimator):
    #TODO: let user choose whether to fit on treatment assignment, outcome or both
    def __init__(
        self,
        treatment_tree_ensemble_estimator,
        target_tree_ensemble_estimator = None,
        #pointwise variance args
        pointwise_variance_dist = 'normal',
        pointwise_variance_alpha = 1,
        pointwise_variance_min_var = 1e-2,
        #knn index args
        n_neighbors=30,
        index_time_params={'M': 30, 'indexThreadQty': 4, 'efConstruction': 100, 'post': 0},
        query_time_params={'efSearch': 100},
        n_jobs = None

    ):

        '''
        Base class for ForestKernel Causal estimator

        Parameters
        ----------

        treatment_tree_ensemble_estimator: Tree ensemble estimator containning the apply method or None

            Model to orthogonalize w.r.t. treatment. if None, the orthogonalization will be performed w.r.t. target only. At least one of
            target_tree_ensemble_estimator or treatment_tree_ensemble_estimator should be passed

        target_tree_ensemble_estimator: Tree ensemble estimator containning the apply method or None. Default = None

            Model to orthogonalize w.r.t. target. if None, the orthogonalization will be performed w.r.t. treatment only. At least one of
            target_tree_ensemble_estimator or treatment_tree_ensemble_estimator should be passed

        pointwise_variance_dist: valid numpy distribution method. Default = 'normal'
            distribution to define pointwise sampling. A point is treated as a distribution centered in its value and its variance will be defined by a function of the Jaccard dissimilarity
            of this point w.r.t. the queried point


        pointwise_variance_alpha: float, Default = 1.0
            alpha parameter for the sampled points. it will spread or concentrate dissimilarities. dist = dist**alpha.

        pointwise_variance_min_var: float, default = 1e-2
            min variance factor (when dissimilarity = 0) for the pointwise sampling

        n_neighbors

        index_time_params

        query_time_params

        n_jobs


        '''
        #tree estimator
        self.treatment_tree_ensemble_estimator = treatment_tree_ensemble_estimator
        self.target_tree_ensemble_estimator = target_tree_ensemble_estimator
        #pointiwse variance for sampling
        self.pointwise_variance_dist = pointwise_variance_dist
        self.pointwise_variance_alpha = pointwise_variance_alpha
        self.pointwise_variance_min_var = pointwise_variance_min_var
        #knn indexing
        self.n_neighbors = n_neighbors
        self.index_time_params = index_time_params
        self.query_time_params = query_time_params
        self.n_jobs = n_jobs
        return

    def fit(self, X, y = None, T = None, save_values = None, **kwargs):

        if (self.treatment_tree_ensemble_estimator is None) and (self.target_tree_ensemble_estimator is None):
            raise ValueError('At least one of [treatment_tree_ensemble_estimator, target_tree_ensemble_estimator] must be not None')
        else:

            if self.treatment_tree_ensemble_estimator is None:
                #only target orthogonalization
                forest_estimator_kernel = JaccardForestKernel(
                    estimator=self.target_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )

                self.forest_estimator_kernel = [
                    forest_estimator_kernel.fit(X, y = y, **kwargs) #matching esstimator regress X on y
                ]
            elif self.target_tree_ensemble_estimator is None:
                #only treatment orthogonalization
                forest_estimator_kernel = JaccardForestKernel(
                    estimator=self.treatment_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )
                self.forest_estimator_kernel = [
                    forest_estimator_kernel.fit(X, y = T, **kwargs) #matching esstimator regress X on T
                ]
            else:
                #orthogonalize on the union of treatment and target orthogonalized points
                treatment_tree_ensemble_estimator = JaccardForestKernel(
                    estimator=self.treatment_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )

                target_forest_estimator_kernel = JaccardForestKernel(
                    estimator=self.target_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )
                self.forest_estimator_kernel = [
                    treatment_tree_ensemble_estimator.fit(X, y = T, **kwargs), #matching esstimator regress X on T
                    target_forest_estimator_kernel.fit(X, y = y, **kwargs) #matching esstimator regress X on y
                ]


        #handle save values
        if not save_values is None:
            if not hasattr(save_values, 'columns'):
                if save_values.ndim == 1:
                    save_values = save_values.reshape(-1,1)
                else:
                    pass
                save_values_columns_ = [f'saved_value_{i}' for i in range(y.shape[-1])]
            else:
                save_values_columns_ = list(save_values.columns)
        else:
            self.save_values_columns_ = []
            save_values = np.empty((y.shape[0], 0), )

        #handle y saving
        if not hasattr(y, 'columns'):
            if y.ndim == 1:
                y = y.reshape(-1,1)
            else:
                pass
            y_columns_ = [f'target_{i}' for i in range(y.shape[-1])]
        else:
            y_columns_ = list(y.columns)

        #handle T saving
        if not hasattr(T, 'columns'):
            if T.ndim == 1:
                T = T.reshape(-1,1)
            else:
                pass
            T_columns_ = [f'treatment_{i}' for i in range(y.shape[-1])]
        else:
            T_columns_ = list(T.columns)

        #cast to array
        save_values = np.array(save_values)
        y = np.array(y)
        T = np.array(T)
        #remove from save_values cols that are already in y or T
        all_save_columns = pd.Series(save_values_columns_ + T_columns_ + y_columns_)
        keep_cols_msk = (~all_save_columns.duplicated()).values
        all_save_columns = all_save_columns[keep_cols_msk].values.tolist()
        #hstack save_values to y and T
        save_values = hstack([save_values, T, y])[:, keep_cols_msk]

        self.saved_values_ = save_values
        self.save_values_columns_ = save_values_columns_
        self.T_columns_ = T_columns_
        self.y_columns_ = y_columns_
        self.all_saved_values_columns_ = all_save_columns
        return self

    #def _saved_data_to_pandas(self, values):
        #list is returned from query, so we have to iterate
    #    return [pd.DataFrame(v, columns = self.all_saved_values_columns_) for v in values]

    def kneighbors(self, X = None, n_neighbors = None, query_from = None):

        if not query_from is None:
            params = self.forest_estimator_kernel.nearest_neighbors_estimator.get_params()
            klass = self.forest_estimator_kernel.nearest_neighbors_estimator.__class__
            knn_estim = klass(**params).fit(X)
            dists, idxs = knn_estim.kneighbors(X = X, n_neighbors = n_neighbors, return_distance = True)
        else:
            knn_estim = self.forest_estimator_kernel
            if len(knn_estim) > 1: #if orthogonalization with both target and treatment
                result = [
                    knn_estim[0].kneighbors(X = X, n_neighbors = n_neighbors//2, return_distance = True),
                    knn_estim[1].kneighbors(X = X, n_neighbors = n_neighbors//2 + n_neighbors%2, return_distance = True),
                ]
                dists = result[0][0], result[1][0]
                idxs = result[0][1], result[1][1]
                dists = [np.hstack([dists[0][i], dists[1][i]]) for i in range(len(X))]
                idxs = [np.hstack([idxs[0][i], idxs[1][i]]) for i in range(len(X))]
            else:
                dists, idxs = knn_estim[0].kneighbors(X = X, n_neighbors = n_neighbors, return_distance = True)

        return dists, idxs

    def query(self, X = None, n_neighbors = None, precomputed_neighbors = None):
        '''
        queries similar datapoints and returns saved_data from queried indexes, alongside
        jaccard dissimilarity
        '''
        if (hasattr(X, 'index') and hasattr(X, 'columns')): #is a dataframe like
            original_X_idxs = X.index
        else:
            original_X_idxs = np.arange(len(X))

        if precomputed_neighbors is None:
            precomputed_neighbors = self.kneighbors(X = X, n_neighbors = n_neighbors)

        dsts, nbrs_idxs = precomputed_neighbors

        values = []
        for i in range(len(nbrs_idxs)):
            msk_i = dsts[i] < 1 #only examples with dissimilarity < 1
            idx_i = nbrs_idxs[i][msk_i]
            dist_i = dsts[i][msk_i].reshape(-1,1)
            v = self.saved_values_[idx_i]
            v = np.hstack([v, dist_i, original_X_idxs[i]*np.ones((len(v),1), dtype = int)])
            #faster than using .assign
            v = pd.DataFrame(v, columns = self.all_saved_values_columns_ + ['dissimilarity', '_index'])
            v['_index'] = v['_index'].astype(int)
            values.append(v)

        return values




# Cell
class CEDTEstimator(_FKEstimator):
    '''
    Continuous Effect - Discrete Treatment estimator
    '''
    def sample(
        self,
        X = None,
        n_neighbors = None,
        precomputed_neighbors = None,
        sample_size = 100,
        alpha = 1,
        noise_type = 'normal',
        variance_mapper='log',
        min_var_factor=0.01,
    ):
        '''
        samples target (y) according to pointwise_variance sampling strategy.
        if not stratify, samples are drawn randomly from neighbors
        '''

        queries = self.query(X, n_neighbors = n_neighbors, precomputed_neighbors = precomputed_neighbors)
        sampled_dfs = []
        for q in queries:
            sampled_df = (
                q.groupby(self.T_columns_)
                .apply(
                    lambda df: np.hstack(
                        [
                            sample_from_neighbors_continuous(
                            neighborhood_values = df[self.y_columns_].values,
                            jac_dists = df['dissimilarity'].values,
                            size = sample_size,
                            alpha = alpha,
                            noise_type = noise_type,
                            variance_mapper=variance_mapper,
                            min_var_factor=min_var_factor,
                            )[0],
                            np.arange(sample_size).reshape(-1,1)
                        ]
                    )
                )
            )

            sampled_df = pd.concat(sampled_df.apply(pd.DataFrame).to_dict())
            sampled_df.columns = self.y_columns_ + ['sample_index']
            sampled_df = sampled_df.reset_index(level = -1, drop = True)
            sampled_df.index = sampled_df.index.set_names(tuple(self.T_columns_))
            sampled_df['sample_index'] = sampled_df['sample_index'].astype(int)
            sampled_df = sampled_df.set_index('sample_index', append = True)
            sampled_dfs.append(sampled_df)

        return sampled_dfs


    def potential_outcomes(
        self,
        X = None,
        n_neighbors = None,
        precomputed_neighbors = None,
        alpha = 1,
        variance_mapper = 'log',
        min_var_factor = 1e-2,
    ):
        '''
        infers conditional average treatment effect for each datapoint, taking the
        correlation between the target and the treatment in each neighborhood
        '''

        q = self.query(X, n_neighbors = n_neighbors, precomputed_neighbors = precomputed_neighbors)
        q = pd.concat(q)

        q = q.groupby(['_index'] + self.T_columns_).apply(
                            lambda df: _agg_mean_variance(
                                df,
                                y_columns = self.y_columns_,
                                alpha = alpha,
                                variance_mapper = variance_mapper,
                                min_var_factor = min_var_factor,
                            )
                        )

        return q

    def treatment_effect(
        self,
        X,
        control,
        n_neighbors = None,
        precomputed_neighbors = None,
        alpha = 1,
        variance_mapper = 'log',
        min_var_factor = 1e-2,

    ):

        outcomes = self.potential_outcomes(
            X = X,
            n_neighbors = n_neighbors,
            precomputed_neighbors = precomputed_neighbors,
            alpha = alpha,
            variance_mapper = variance_mapper,
            min_var_factor = min_var_factor,
        )

        #control = 0
        if not isinstance(control, (tuple, list, set)):
            control = (control,)

        c_idx_mean = slice(None), *control, 'mean'
        t_idx_mean = slice(None), *[slice(None)]*len(control),'mean'

        c_idx_var = slice(None), *control, 'variance'
        t_idx_var = slice(None), *[slice(None)]*len(control),'variance'

        means = outcomes.loc[t_idx_mean] - outcomes.loc[c_idx_mean].reset_index(level = [*range(1,len(c_idx_mean))], drop = True)
        var = outcomes.loc[t_idx_var] + outcomes.loc[c_idx_var].reset_index(level = [*range(1,len(c_idx_mean))], drop = True)

        var['statistic'] = 'variance'
        means['statistic'] = 'mean'

        #var = var.set_index('statistic', append = True)
        #means = means.set_index('statistic', append = True)

        outcomes.loc[var.index] = var
        outcomes.loc[means.index] = means
        return outcomes

# Cell
import multiprocessing
from multiprocessing import Pool, cpu_count
from functools import partial

import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator, RegressorMixin
import statsmodels.api as sm

from heartwood.kernel import JaccardForestKernel
from .utils import hstack, vstack
from .dist import sample_from_neighbors_continuous, estimate_mean_and_variance_from_neighbors_mixture

# Cell

def pandas_gorupby_apply_parallel(dfGrouped, func, **kwargs):
    with Pool(cpu_count()) as p:
        ret_list = p.map(partial(func, **kwargs), [group for name, group in dfGrouped])
    return ret_list

def idxs_split(a, n, axis = 0, drop_empty = True, return_slices = False):
    '''
    returns indices of the n splits of the a array along axis
    '''
    if not hasattr(a, 'shape'):
        a = np.arange(len(a))
    idxs = np.array_split(np.arange(a.shape[axis]), n, axis = 0)
    idxs = [list(i) for i in idxs]

    if drop_empty:
        idxs = [i for i in idxs if i]

    if return_slices:
        idxs = [slice(min(i), max(i)+1) for i in idxs if i]

    return idxs

def _agg_covariance(df, y_columns, T_columns, alpha):
    '''
    covariance of a given target sample in relation to some given Treatment
    '''

    import statsmodels.api as sm
    w = (1 - df['dissimilarity'])**alpha
    model = sm.WLS(df[y_columns],df[T_columns],w).fit()

    std = model.bse
    #std.columns = ['std' for _ in std.columns]
    slope = model.params
    #slope.columns = ['slope' for _ in slope.columns]
    count = (slope*0+1)*df.shape[0]
    pvalues = model.pvalues


    d = [slope,std,count, pvalues]

    idx = pd.Index(['slope','std','count','p_value'], name = 'statistic')
    d = pd.DataFrame(d, index = idx)


    return d

def _agg_mean_variance(df, y_columns, alpha, variance_mapper, min_var_factor):
    '''
    calculates mean and variance for a given sample in groupby
    '''
    mean, var = estimate_mean_and_variance_from_neighbors_mixture(
        df[y_columns].values,
        df['dissimilarity'].values,
        alpha = alpha,
        variance_mapper = variance_mapper,
        min_var_factor = min_var_factor,
    )

    count = df.shape[0]
    if var.ndim > 1:

        #std = var.diagonal()**(1/2)
        d = np.concatenate([mean,var,count*np.ones(mean.shape)]).reshape(3,len(y_columns))
    else:
        #std = var**1/2
        d = [mean,var,count]

    d = pd.DataFrame(d)


    d.index = pd.Index(['mean','variance','count'], name = 'statistic')
    d.columns = y_columns

    return d

# Cell
class SMFormulaWrapper(BaseEstimator, RegressorMixin):
    """
    A sklearn-style wrapper for formula-based statsmodels regressors,
    thanks to https://nelsonauner.com/data/2018/05/21/wrap-statsmodels-in-sklearn.html
    """
    def __init__(self, model_class, formula):
        self.model_class = model_class
        self.formula = formula
    def fit(self, X, y=None):
        self.model_ = self.model_class(self.formula, data=X)
        self.results_ = self.model_.fit()
    def predict(self, X):
        return self.results_.predict(X)

class SMWrapper(BaseEstimator, RegressorMixin):
    """
    A universal sklearn-style wrapper for statsmodels regressors ,
    thanks to https://stackoverflow.com/questions/41045752/using-statsmodel-estimations-with-scikit-learn-cross-validation-is-it-possible/
    """
    def __init__(self, model_class, fit_intercept=True):
        self.model_class = model_class
        self.fit_intercept = fit_intercept
    def fit(self, X, y):
        if self.fit_intercept:
            X = sm.add_constant(X)
        self.model_ = self.model_class(y, X)
        self.results_ = self.model_.fit()
        return self
    def predict(self, X):
        if self.fit_intercept:
            X = sm.add_constant(X)
        return self.results_.predict(X)

# Cell
class _FKEstimator(BaseEstimator):
    #TODO: let user choose whether to fit on treatment assignment, outcome or both
    def __init__(
        self,
        treatment_tree_ensemble_estimator,
        target_tree_ensemble_estimator = None,
        #pointwise variance args
        pointwise_variance_dist = 'normal',
        pointwise_variance_alpha = 1,
        pointwise_variance_min_var = 1e-2,
        #knn index args
        n_neighbors=30,
        index_time_params={'M': 30, 'indexThreadQty': 4, 'efConstruction': 100, 'post': 0},
        query_time_params={'efSearch': 100},
        n_jobs = None

    ):

        '''
        Base class for ForestKernel Causal estimator

        Parameters
        ----------

        treatment_tree_ensemble_estimator: Tree ensemble estimator containning the apply method or None

            Model to orthogonalize w.r.t. treatment. if None, the orthogonalization will be performed w.r.t. target only. At least one of
            target_tree_ensemble_estimator or treatment_tree_ensemble_estimator should be passed

        target_tree_ensemble_estimator: Tree ensemble estimator containning the apply method or None. Default = None

            Model to orthogonalize w.r.t. target. if None, the orthogonalization will be performed w.r.t. treatment only. At least one of
            target_tree_ensemble_estimator or treatment_tree_ensemble_estimator should be passed

        pointwise_variance_dist: valid numpy distribution method. Default = 'normal'
            distribution to define pointwise sampling. A point is treated as a distribution centered in its value and its variance will be defined by a function of the Jaccard dissimilarity
            of this point w.r.t. the queried point


        pointwise_variance_alpha: float, Default = 1.0
            alpha parameter for the sampled points. it will spread or concentrate dissimilarities. dist = dist**alpha.

        pointwise_variance_min_var: float, default = 1e-2
            min variance factor (when dissimilarity = 0) for the pointwise sampling

        n_neighbors

        index_time_params

        query_time_params

        n_jobs


        '''
        #tree estimator
        self.treatment_tree_ensemble_estimator = treatment_tree_ensemble_estimator
        self.target_tree_ensemble_estimator = target_tree_ensemble_estimator
        #pointiwse variance for sampling
        self.pointwise_variance_dist = pointwise_variance_dist
        self.pointwise_variance_alpha = pointwise_variance_alpha
        self.pointwise_variance_min_var = pointwise_variance_min_var
        #knn indexing
        self.n_neighbors = n_neighbors
        self.index_time_params = index_time_params
        self.query_time_params = query_time_params
        self.n_jobs = n_jobs
        return

    def fit(self, X, y = None, T = None, save_values = None, **kwargs):

        if (self.treatment_tree_ensemble_estimator is None) and (self.target_tree_ensemble_estimator is None):
            raise ValueError('At least one of [treatment_tree_ensemble_estimator, target_tree_ensemble_estimator] must be not None')
        else:

            if self.treatment_tree_ensemble_estimator is None:
                #only target orthogonalization
                forest_estimator_kernel = JaccardForestKernel(
                    estimator=self.target_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )

                self.forest_estimator_kernel = [
                    forest_estimator_kernel.fit(X, y = y, **kwargs) #matching esstimator regress X on y
                ]
            elif self.target_tree_ensemble_estimator is None:
                #only treatment orthogonalization
                forest_estimator_kernel = JaccardForestKernel(
                    estimator=self.treatment_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )
                self.forest_estimator_kernel = [
                    forest_estimator_kernel.fit(X, y = T, **kwargs) #matching esstimator regress X on T
                ]
            else:
                #orthogonalize on the union of treatment and target orthogonalized points
                treatment_tree_ensemble_estimator = JaccardForestKernel(
                    estimator=self.treatment_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )

                target_forest_estimator_kernel = JaccardForestKernel(
                    estimator=self.target_tree_ensemble_estimator,
                    n_neighbors=self.n_neighbors,
                    index_time_params=self.index_time_params,
                    query_time_params=self.query_time_params,
                )
                self.forest_estimator_kernel = [
                    treatment_tree_ensemble_estimator.fit(X, y = T, **kwargs), #matching esstimator regress X on T
                    target_forest_estimator_kernel.fit(X, y = y, **kwargs) #matching esstimator regress X on y
                ]


        #handle save values
        if not save_values is None:
            if not hasattr(save_values, 'columns'):
                if save_values.ndim == 1:
                    save_values = save_values.reshape(-1,1)
                else:
                    pass
                save_values_columns_ = [f'saved_value_{i}' for i in range(y.shape[-1])]
            else:
                save_values_columns_ = list(save_values.columns)
        else:
            self.save_values_columns_ = []
            save_values = np.empty((y.shape[0], 0), )

        #handle y saving
        if not hasattr(y, 'columns'):
            if y.ndim == 1:
                y = y.reshape(-1,1)
            else:
                pass
            y_columns_ = [f'target_{i}' for i in range(y.shape[-1])]
        else:
            y_columns_ = list(y.columns)

        #handle T saving
        if not hasattr(T, 'columns'):
            if T.ndim == 1:
                T = T.reshape(-1,1)
            else:
                pass
            T_columns_ = [f'treatment_{i}' for i in range(y.shape[-1])]
        else:
            T_columns_ = list(T.columns)

        #cast to array
        save_values = np.array(save_values)
        y = np.array(y)
        T = np.array(T)
        #remove from save_values cols that are already in y or T
        all_save_columns = pd.Series(save_values_columns_ + T_columns_ + y_columns_)
        keep_cols_msk = (~all_save_columns.duplicated()).values
        all_save_columns = all_save_columns[keep_cols_msk].values.tolist()
        #hstack save_values to y and T
        save_values = hstack([save_values, T, y])[:, keep_cols_msk]

        self.saved_values_ = save_values
        self.save_values_columns_ = save_values_columns_
        self.T_columns_ = T_columns_
        self.y_columns_ = y_columns_
        self.all_saved_values_columns_ = all_save_columns
        return self

    #def _saved_data_to_pandas(self, values):
        #list is returned from query, so we have to iterate
    #    return [pd.DataFrame(v, columns = self.all_saved_values_columns_) for v in values]

    def kneighbors(self, X = None, n_neighbors = None, query_from = None):

        if not query_from is None:
            params = self.forest_estimator_kernel.nearest_neighbors_estimator.get_params()
            klass = self.forest_estimator_kernel.nearest_neighbors_estimator.__class__
            knn_estim = klass(**params).fit(X)
            dists, idxs = knn_estim.kneighbors(X = X, n_neighbors = n_neighbors, return_distance = True)
        else:
            knn_estim = self.forest_estimator_kernel
            if len(knn_estim) > 1: #if orthogonalization with both target and treatment
                result = [
                    knn_estim[0].kneighbors(X = X, n_neighbors = n_neighbors//2, return_distance = True),
                    knn_estim[1].kneighbors(X = X, n_neighbors = n_neighbors//2 + n_neighbors%2, return_distance = True),
                ]
                dists = result[0][0], result[1][0]
                idxs = result[0][1], result[1][1]
                dists = [np.hstack([dists[0][i], dists[1][i]]) for i in range(len(X))]
                idxs = [np.hstack([idxs[0][i], idxs[1][i]]) for i in range(len(X))]
            else:
                dists, idxs = knn_estim[0].kneighbors(X = X, n_neighbors = n_neighbors, return_distance = True)

        return dists, idxs

    def query(self, X = None, n_neighbors = None, precomputed_neighbors = None):
        '''
        queries similar datapoints and returns saved_data from queried indexes, alongside
        jaccard dissimilarity
        '''
        if (hasattr(X, 'index') and hasattr(X, 'columns')): #is a dataframe like
            original_X_idxs = X.index
        else:
            original_X_idxs = np.arange(len(X))

        if precomputed_neighbors is None:
            precomputed_neighbors = self.kneighbors(X = X, n_neighbors = n_neighbors)

        dsts, nbrs_idxs = precomputed_neighbors

        values = []
        for i in range(len(nbrs_idxs)):
            msk_i = dsts[i] < 1 #only examples with dissimilarity < 1
            idx_i = nbrs_idxs[i][msk_i]
            dist_i = dsts[i][msk_i].reshape(-1,1)
            v = self.saved_values_[idx_i]
            v = np.hstack([v, dist_i, original_X_idxs[i]*np.ones((len(v),1), dtype = int)])
            #faster than using .assign
            v = pd.DataFrame(v, columns = self.all_saved_values_columns_ + ['dissimilarity', '_index'])
            v['_index'] = v['_index'].astype(int)
            values.append(v)

        return values




# Cell
class CEDTEstimator(_FKEstimator):
    '''
    Continuous Effect - Discrete Treatment estimator
    '''
    def sample(
        self,
        X = None,
        n_neighbors = None,
        precomputed_neighbors = None,
        sample_size = 100,
        alpha = 1,
        noise_type = 'normal',
        variance_mapper='log',
        min_var_factor=0.01,
    ):
        '''
        samples target (y) according to pointwise_variance sampling strategy.
        if not stratify, samples are drawn randomly from neighbors
        '''

        queries = self.query(X, n_neighbors = n_neighbors, precomputed_neighbors = precomputed_neighbors)
        sampled_dfs = []
        for q in queries:
            sampled_df = (
                q.groupby(self.T_columns_)
                .apply(
                    lambda df: np.hstack(
                        [
                            sample_from_neighbors_continuous(
                            neighborhood_values = df[self.y_columns_].values,
                            jac_dists = df['dissimilarity'].values,
                            size = sample_size,
                            alpha = alpha,
                            noise_type = noise_type,
                            variance_mapper=variance_mapper,
                            min_var_factor=min_var_factor,
                            )[0],
                            np.arange(sample_size).reshape(-1,1)
                        ]
                    )
                )
            )

            sampled_df = pd.concat(sampled_df.apply(pd.DataFrame).to_dict())
            sampled_df.columns = self.y_columns_ + ['sample_index']
            sampled_df = sampled_df.reset_index(level = -1, drop = True)
            sampled_df.index = sampled_df.index.set_names(tuple(self.T_columns_))
            sampled_df['sample_index'] = sampled_df['sample_index'].astype(int)
            sampled_df = sampled_df.set_index('sample_index', append = True)
            sampled_dfs.append(sampled_df)

        return sampled_dfs


    def potential_outcomes(
        self,
        X = None,
        n_neighbors = None,
        precomputed_neighbors = None,
        alpha = 1,
        variance_mapper = 'log',
        min_var_factor = 1e-2,
    ):
        '''
        infers conditional average treatment effect for each datapoint, taking the
        correlation between the target and the treatment in each neighborhood
        '''

        q = self.query(X, n_neighbors = n_neighbors, precomputed_neighbors = precomputed_neighbors)
        q = pd.concat(q)

        q = q.groupby(['_index'] + self.T_columns_).apply(
                            lambda df: _agg_mean_variance(
                                df,
                                y_columns = self.y_columns_,
                                alpha = alpha,
                                variance_mapper = variance_mapper,
                                min_var_factor = min_var_factor,
                            )
                        )

        return q

    def treatment_effect(
        self,
        X,
        control,
        n_neighbors = None,
        precomputed_neighbors = None,
        alpha = 1,
        variance_mapper = 'log',
        min_var_factor = 1e-2,

    ):

        outcomes = self.potential_outcomes(
            X = X,
            n_neighbors = n_neighbors,
            precomputed_neighbors = precomputed_neighbors,
            alpha = alpha,
            variance_mapper = variance_mapper,
            min_var_factor = min_var_factor,
        )

        #control = 0
        if not isinstance(control, (tuple, list, set)):
            control = (control,)

        c_idx_mean = slice(None), *control, 'mean'
        t_idx_mean = slice(None), *[slice(None)]*len(control),'mean'

        c_idx_var = slice(None), *control, 'variance'
        t_idx_var = slice(None), *[slice(None)]*len(control),'variance'

        means = outcomes.loc[t_idx_mean] - outcomes.loc[c_idx_mean].reset_index(level = [*range(1,len(c_idx_mean))], drop = True)
        var = outcomes.loc[t_idx_var] + outcomes.loc[c_idx_var].reset_index(level = [*range(1,len(c_idx_mean))], drop = True)

        var['statistic'] = 'variance'
        means['statistic'] = 'mean'

        #var = var.set_index('statistic', append = True)
        #means = means.set_index('statistic', append = True)

        outcomes.loc[var.index] = var
        outcomes.loc[means.index] = means
        return outcomes