# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/dist.ipynb (unless otherwise specified).

__all__ = ['get_distribution_var_factor_jaccard', 'pointwise_variance',
           'estimate_mean_and_variance_from_neighbors_mixture', 'sample_from_neighbors_continuous', 'PointwiseMixture',
           'JaccardPointwiseGaussianMixture', 'get_distribution_var_factor_jaccard', 'pointwise_variance',
           'estimate_mean_and_variance_from_neighbors_mixture', 'sample_from_neighbors_continuous', 'PointwiseMixture',
           'JaccardPointwiseGaussianMixture']

# Cell
import numpy as np
from scipy import stats

from sklearn.base import DensityMixin, BaseEstimator
from functools import partial
from sklearn.utils import check_array


# Cell
def get_distribution_var_factor_jaccard(jac_dists, min_var_factor = 1e-2, alpha = 1, func = 'log'):
    '''
    gets the variance factor of the "point distribution" given a jaccard distance from
    the query point

    to get the actual variance, use the variance factor alongside the variance of the
    observed variable in the neighbor points.

    example:

    get the 30 nearest neighbors, calculat their variance along some axis,
    then calculate the variance factor of each point and then multiply to get the
    "point variance in the contribution" for each point

    functions to test:
    tangent(x), 1/x, min_var - log(1-x)
    '''
    funcs = ['log', 'inverse_dist', 'constant']
    assert min_var_factor > 0
    assert alpha > 0

    #if sim = 1, var_factor = min_var_factor, if sim -> 0, var_factor -> inf
    if func == 'log':
        var_factor = min_var_factor - alpha*np.log(1-jac_dists)
    elif func == 'inverse_dist':
        var_factor = min_var_factor/(1-jac_dists)**alpha
    elif func == 'constant':
        var_factor = np.ones(jac_dist.shape)
    return var_factor

def pointwise_variance(values, jac_dists, min_var_factor = 1e-2, alpha = 1, variance_mapper = 'log'):
    '''
    gets the pointwise variance for each neighbor of a given point, considering
    the variance of the neighborhood and the variance factor of each point

    each point contributes to the estimation of the queried point, but this contribution
    has a variance associated with how much this point is alike the queried point. in this sense
    pointwise variance is defined, as the variance of the contribution of each point, given the
    variance of all the points and how similar the points in the neighborhood are to the queried point

    the variance of the contribution os different from the (sampling)weight of the point,
    since points with lower similarity will have a higher variance, but will be less likely to be sampled
    '''

    #global variance is calculated with the wieghted full neighbors instead of the sampled values, in order to avoid 0 variance
    #if a single value is sampled

    #expand values if contains only one row, so np.cov won't break
    if values.shape[0] < 2:
        values = np.concatenate([values,values])
        jac_dists = np.concatenate([jac_dists,jac_dists])

    var_factor = get_distribution_var_factor_jaccard(jac_dists, min_var_factor, alpha, func = variance_mapper)
    var = np.cov(values, rowvar = False, aweights=(1- jac_dists.flatten())**alpha) #global variance weights are proportional to point similarities, not distance
    if var.ndim > 1:
        broadcasted_var_factor = jac_dists.reshape(jac_dists.shape[0],1)[:, np.newaxis]
        pointwise_variance = broadcasted_var_factor*var
    else:
        pointwise_variance = var_factor*var

    return pointwise_variance

# Cell
def estimate_mean_and_variance_from_neighbors_mixture(
    neighborhood_values,
    jac_dists,
    alpha = 1,
    variance_mapper = 'log',
    min_var_factor = 1e-2,
):
    '''
    refs
    https://stats.stackexchange.com/questions/16608/what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
    https://math.stackexchange.com/questions/195911/calculation-of-the-covariance-of-gaussian-mixtures

    at the end of the day, impelmented covaraince scaling according to scaling in the diagonal of covariance matrix
    for multidimensional case
    '''

    weights = ((1-jac_dists)**alpha)
    weights = weights/weights.sum(axis = 0) #normalize

    neighborhood_values = check_array(neighborhood_values)

    if neighborhood_values.shape[-1] == 1:
        neighborhood_values = neighborhood_values.flatten()

    mean = np.average(neighborhood_values, weights = weights, axis = 0)

    pw_var = pointwise_variance(
        values = neighborhood_values,
        jac_dists = jac_dists,
        min_var_factor = min_var_factor,
        alpha = alpha,
        variance_mapper = variance_mapper
    )


    #handle broadcasting
    extra_dims_n = pw_var.ndim - neighborhood_values.ndim
    extra_dims_w = pw_var.ndim - weights.ndim

    if extra_dims_n > 0:
        neighborhood_values = neighborhood_values.reshape(*neighborhood_values.shape, *extra_dims_n*[1])
    if extra_dims_w > 0:
        weights = weights.reshape(*weights.shape, *extra_dims_w*[1])



    #var = np.sum(weights*pw_var, axis = 0)
    wvar = np.sum(weights*pw_var, axis = 0)
    var = (
        wvar+
        np.sum(weights*(neighborhood_values**2), axis = 0) -
        mean**2
    )

    if var.ndim > 1:

        cov = np.sum(weights*pw_var, axis = 0)
        factor = var.diagonal()/cov.diagonal() #scale caovariances by variance scaling factor
        cov = cov*factor
        np.fill_diagonal(cov, var.diagonal())
        var = cov

    return mean, var


def sample_from_neighbors_continuous(
    neighborhood_values,
    jac_dists,
    size = 100,
    noise_type = 'normal',
    alpha = 1,
    scale_variance_pointwise = True,
    variance_mapper = 'log',
    min_var_factor = 1e-2,
):
    '''
    samples from neighbor points with noise

    returns (samples, idxs)
    '''
    #reshape values
    if neighborhood_values.ndim <= 1:
        neighborhood_values = neighborhood_values.reshape(-1,1)

    #handle noise_type
    valid_noise_types = ['normal', 'multivariate_normal', None]
    if not noise_type in valid_noise_types:
        raise ValueError(f'noise_type should be one of {valid_noise_types}, got {noise_type}')
    else:
        if noise_type == 'normal':
            var_preprocess = np.sqrt
            if not neighborhood_values.shape[-1] == 1:
                raise ValueError(f'for "normal" noise_type, data should be 1d, got shape {neighborhood_values.shape}')
        elif noise_type == 'multivariate_normal':
            var_preprocess = lambda x: x
            if not neighborhood_values.shape[-1] > 1:
                raise ValueError(f'for "multivariate_normal" noise_type, data should be N-dimensional for N >1, got shape {neighborhood_values.shape}')


    #apply alppha and l1-normalize sample weights
    sample_weights = (1- jac_dists)**alpha #transform distance into similarity with 1 - jac_d
    sample_weights = sample_weights/sample_weights.sum()
    #sample based on sample weights
    sampled_idxs = np.random.choice(np.arange(neighborhood_values.shape[0]),size = size, p = sample_weights, replace = True)
    #sampled_idxs = np.sort(sampled_idxs)

    if (not noise_type is None) and (len(neighborhood_values) > 1):
        #adds noise accordingly
        pw_var = pointwise_variance(neighborhood_values, jac_dists = jac_dists, min_var_factor = min_var_factor, alpha = alpha)
        counts = np.bincount(sampled_idxs)
        msk = counts > 0
        unique, counts = np.arange(len(counts))[msk], counts[msk]
        noise_type = getattr(np.random, noise_type)
        if pw_var.ndim > 1:
            ndims = pw_var.shape[-1]
            mean = np.array([0]*ndims)
        else:
            mean = 0

        samples = []
        for i in range(unique.shape[0]):
            idx = unique[i]
            var = pw_var[idx]
            var = var_preprocess(var)
            noise = noise_type(mean, var, size = counts[i])
            sampled_values = neighborhood_values[idx] + noise
            #samples = np.vstack([samples, sampled_values])
            samples.append(sampled_values)


        if sampled_values.ndim > 1:
            samples = np.vstack(samples)
        else:
            samples = np.vstack([s.reshape(-1,1) for s in samples])
    else:
        #sample without adding noise
        samples = neighborhood_values[sampled_idxs]

    return samples, sampled_idxs


# Cell
class PointwiseMixture(DensityMixin, BaseEstimator):

    def __init__(self, distribution = 'normal'):
        self.distribution = distribution
        return

    def sample(self, size = 1):
        '''
        draw samples from mixture
        '''
        idxs = np.random.choice(np.arange(len(self.dists_)), size = size, replace = True, p = self.weights_)
        counts = np.bincount(idxs)
        msk = counts > 0
        unique, counts = np.arange(len(counts))[msk], counts[msk]

        samples = []
        for i in range(len(unique)):
            idx = unique[i]
            n_samples = counts[i]
            sampled_values = self.dists_[idx](size = n_samples)
            samples.append(sampled_values)

        if sampled_values.ndim > 1:
            samples = np.vstack(samples)
        else:
            samples = np.vstack([s.reshape(-1,1) for s in samples])

        return samples

    def fit(self, loc, std, weights = None):
        '''
        fits a mixture estimating pointwise varainces from proposed method
        '''

        if weights is None:
            weights = np.ones((len(loc),))

        if not loc.ndim == 1:
            raise ValueError(f'loc should be 1d, got shape {loc.shape}')
        if not weights.ndim == 1:
            raise ValueError(f'weights should be 1d, got shape {weights.shape}')
        if not std.ndim == 1:
            raise ValueError(f'std should be 1d, got shape {std.shape}')

        ndim = 1
        weights_ = weights/weights.sum()

        self.dists_ = [partial(np.random.normal, loc = loc[i], scale = std[i]) for i in range(len(loc))]

        self.weights_ = weights_
        self.n_dim_ = ndim
        #self.means_ = X
        #self.covariances_ = pw_variances
        return self


class JaccardPointwiseGaussianMixture(DensityMixin, BaseEstimator):

    def __init__(
        self,
        pointwse_distribution = 'normal',
        alpha = 1,
        scale_variance_pointwise = True,
        variance_mapper = 'log',
        min_var_factor = 1e-2,
    ):

        self.pointwse_distribution = pontiwse_distribution
        self.alpha = alpha
        self.scale_variance_pointwise = scale_variance_pointwise
        self.variance_mapper = variance_mapper
        self.min_var_factor = min_var_factor
        return

    def fit(self, X, jac_dists = None, **kwargs):
        '''
        fits a mixture estimating pointwise varainces from proposed method
        '''

        X = check_array(
            X,
            force_all_finite = True,
            ensure_2d = True
        )

        if X.shape[-1] == 1:
            X = X.flatten()
            ndim = 1
        else:
            ndim = X.shape[-1]

        #estimate global mean and cov
        pw_variances = pointwise_variance(
            values = X,
            jac_dists = jac_dists,
            min_var_factor = self.min_var_factor,
            alpha = self.alpha,
            variance_mapper = self.variance_mapper
        )

        weights_ = (1 - jac_dists)**self.alpha
        weights_ = weights_/weights_.sum()

        if ndim == 1:
            pointwise_std = np.sqrt(pw_variances)
            self.dists_ = [partial(np.random.normal, loc = X[i], scale = pointwise_std[i]) for i in range(len(X))]
        else:
            self.dists_ = [partial(np.random.multivariate_normal, mean = X[i], cov = pw_variances[i]) for i in range(len(X))]

        self.weights_ = weights_
        self.n_dim_ = ndim
        #self.means_ = X
        #self.covariances_ = pw_variances
        return self

    def sample(self, size = 1):
        '''
        draw samples from mixture
        '''
        idxs = np.random.choice(np.arange(len(self.dists_)), size = size, replace = True, p = self.weights_)
        counts = np.bincount(idxs)
        msk = counts > 0
        unique, counts = np.arange(len(counts))[msk], counts[msk]

        samples = []
        for i in range(len(unique)):
            idx = unique[i]
            n_samples = counts[i]
            sampled_values = self.dists_[idx](size = n_samples)
            samples.append(sampled_values)

        if sampled_values.ndim > 1:
            samples = np.vstack(samples)
        else:
            samples = np.vstack([s.reshape(-1,1) for s in samples])

        return samples

# Cell
import numpy as np
from scipy import stats

from sklearn.base import DensityMixin, BaseEstimator
from functools import partial
from sklearn.utils import check_array


# Cell
def get_distribution_var_factor_jaccard(jac_dists, min_var_factor = 1e-2, alpha = 1, func = 'log'):
    '''
    gets the variance factor of the "point distribution" given a jaccard distance from
    the query point

    to get the actual variance, use the variance factor alongside the variance of the
    observed variable in the neighbor points.

    example:

    get the 30 nearest neighbors, calculat their variance along some axis,
    then calculate the variance factor of each point and then multiply to get the
    "point variance in the contribution" for each point

    functions to test:
    tangent(x), 1/x, min_var - log(1-x)
    '''
    funcs = ['log', 'inverse_dist', 'constant']
    assert min_var_factor > 0
    assert alpha > 0

    #if sim = 1, var_factor = min_var_factor, if sim -> 0, var_factor -> inf
    if func == 'log':
        var_factor = min_var_factor - alpha*np.log(1-jac_dists)
    elif func == 'inverse_dist':
        var_factor = min_var_factor/(1-jac_dists)**alpha
    elif func == 'constant':
        var_factor = np.ones(jac_dist.shape)
    return var_factor

def pointwise_variance(values, jac_dists, min_var_factor = 1e-2, alpha = 1, variance_mapper = 'log'):
    '''
    gets the pointwise variance for each neighbor of a given point, considering
    the variance of the neighborhood and the variance factor of each point

    each point contributes to the estimation of the queried point, but this contribution
    has a variance associated with how much this point is alike the queried point. in this sense
    pointwise variance is defined, as the variance of the contribution of each point, given the
    variance of all the points and how similar the points in the neighborhood are to the queried point

    the variance of the contribution os different from the (sampling)weight of the point,
    since points with lower similarity will have a higher variance, but will be less likely to be sampled
    '''

    #global variance is calculated with the wieghted full neighbors instead of the sampled values, in order to avoid 0 variance
    #if a single value is sampled

    #expand values if contains only one row, so np.cov won't break
    if values.shape[0] < 2:
        values = np.concatenate([values,values])
        jac_dists = np.concatenate([jac_dists,jac_dists])

    var_factor = get_distribution_var_factor_jaccard(jac_dists, min_var_factor, alpha, func = variance_mapper)
    var = np.cov(values, rowvar = False, aweights=(1- jac_dists.flatten())**alpha) #global variance weights are proportional to point similarities, not distance
    if var.ndim > 1:
        broadcasted_var_factor = jac_dists.reshape(jac_dists.shape[0],1)[:, np.newaxis]
        pointwise_variance = broadcasted_var_factor*var
    else:
        pointwise_variance = var_factor*var

    return pointwise_variance

# Cell
def estimate_mean_and_variance_from_neighbors_mixture(
    neighborhood_values,
    jac_dists,
    alpha = 1,
    variance_mapper = 'log',
    min_var_factor = 1e-2,
):
    '''
    refs
    https://stats.stackexchange.com/questions/16608/what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
    https://math.stackexchange.com/questions/195911/calculation-of-the-covariance-of-gaussian-mixtures

    at the end of the day, impelmented covaraince scaling according to scaling in the diagonal of covariance matrix
    for multidimensional case
    '''

    weights = ((1-jac_dists)**alpha)
    weights = weights/weights.sum(axis = 0) #normalize

    neighborhood_values = check_array(neighborhood_values)

    if neighborhood_values.shape[-1] == 1:
        neighborhood_values = neighborhood_values.flatten()

    mean = np.average(neighborhood_values, weights = weights, axis = 0)

    pw_var = pointwise_variance(
        values = neighborhood_values,
        jac_dists = jac_dists,
        min_var_factor = min_var_factor,
        alpha = alpha,
        variance_mapper = variance_mapper
    )


    #handle broadcasting
    extra_dims_n = pw_var.ndim - neighborhood_values.ndim
    extra_dims_w = pw_var.ndim - weights.ndim

    if extra_dims_n > 0:
        neighborhood_values = neighborhood_values.reshape(*neighborhood_values.shape, *extra_dims_n*[1])
    if extra_dims_w > 0:
        weights = weights.reshape(*weights.shape, *extra_dims_w*[1])



    #var = np.sum(weights*pw_var, axis = 0)
    wvar = np.sum(weights*pw_var, axis = 0)
    var = (
        wvar+
        np.sum(weights*(neighborhood_values**2), axis = 0) -
        mean**2
    )

    if var.ndim > 1:

        cov = np.sum(weights*pw_var, axis = 0)
        factor = var.diagonal()/cov.diagonal() #scale caovariances by variance scaling factor
        cov = cov*factor
        np.fill_diagonal(cov, var.diagonal())
        var = cov

    return mean, var


def sample_from_neighbors_continuous(
    neighborhood_values,
    jac_dists,
    size = 100,
    noise_type = 'normal',
    alpha = 1,
    scale_variance_pointwise = True,
    variance_mapper = 'log',
    min_var_factor = 1e-2,
):
    '''
    samples from neighbor points with noise

    returns (samples, idxs)
    '''
    #reshape values
    if neighborhood_values.ndim <= 1:
        neighborhood_values = neighborhood_values.reshape(-1,1)

    #handle noise_type
    valid_noise_types = ['normal', 'multivariate_normal', None]
    if not noise_type in valid_noise_types:
        raise ValueError(f'noise_type should be one of {valid_noise_types}, got {noise_type}')
    else:
        if noise_type == 'normal':
            var_preprocess = np.sqrt
            if not neighborhood_values.shape[-1] == 1:
                raise ValueError(f'for "normal" noise_type, data should be 1d, got shape {neighborhood_values.shape}')
        elif noise_type == 'multivariate_normal':
            var_preprocess = lambda x: x
            if not neighborhood_values.shape[-1] > 1:
                raise ValueError(f'for "multivariate_normal" noise_type, data should be N-dimensional for N >1, got shape {neighborhood_values.shape}')


    #apply alppha and l1-normalize sample weights
    sample_weights = (1- jac_dists)**alpha #transform distance into similarity with 1 - jac_d
    sample_weights = sample_weights/sample_weights.sum()
    #sample based on sample weights
    sampled_idxs = np.random.choice(np.arange(neighborhood_values.shape[0]),size = size, p = sample_weights, replace = True)
    #sampled_idxs = np.sort(sampled_idxs)

    if (not noise_type is None) and (len(neighborhood_values) > 1):
        #adds noise accordingly
        pw_var = pointwise_variance(neighborhood_values, jac_dists = jac_dists, min_var_factor = min_var_factor, alpha = alpha)
        counts = np.bincount(sampled_idxs)
        msk = counts > 0
        unique, counts = np.arange(len(counts))[msk], counts[msk]
        noise_type = getattr(np.random, noise_type)
        if pw_var.ndim > 1:
            ndims = pw_var.shape[-1]
            mean = np.array([0]*ndims)
        else:
            mean = 0

        samples = []
        for i in range(unique.shape[0]):
            idx = unique[i]
            var = pw_var[idx]
            var = var_preprocess(var)
            noise = noise_type(mean, var, size = counts[i])
            sampled_values = neighborhood_values[idx] + noise
            #samples = np.vstack([samples, sampled_values])
            samples.append(sampled_values)


        if sampled_values.ndim > 1:
            samples = np.vstack(samples)
        else:
            samples = np.vstack([s.reshape(-1,1) for s in samples])
    else:
        #sample without adding noise
        samples = neighborhood_values[sampled_idxs]

    return samples, sampled_idxs


# Cell
class PointwiseMixture(DensityMixin, BaseEstimator):

    def __init__(self, distribution = 'normal'):
        self.distribution = distribution
        return

    def sample(self, size = 1):
        '''
        draw samples from mixture
        '''
        idxs = np.random.choice(np.arange(len(self.dists_)), size = size, replace = True, p = self.weights_)
        counts = np.bincount(idxs)
        msk = counts > 0
        unique, counts = np.arange(len(counts))[msk], counts[msk]

        samples = []
        for i in range(len(unique)):
            idx = unique[i]
            n_samples = counts[i]
            sampled_values = self.dists_[idx](size = n_samples)
            samples.append(sampled_values)

        if sampled_values.ndim > 1:
            samples = np.vstack(samples)
        else:
            samples = np.vstack([s.reshape(-1,1) for s in samples])

        return samples

    def fit(self, loc, std, weights = None):
        '''
        fits a mixture estimating pointwise varainces from proposed method
        '''

        if weights is None:
            weights = np.ones((len(loc),))

        if not loc.ndim == 1:
            raise ValueError(f'loc should be 1d, got shape {loc.shape}')
        if not weights.ndim == 1:
            raise ValueError(f'weights should be 1d, got shape {weights.shape}')
        if not std.ndim == 1:
            raise ValueError(f'std should be 1d, got shape {std.shape}')

        ndim = 1
        weights_ = weights/weights.sum()

        self.dists_ = [partial(np.random.normal, loc = loc[i], scale = std[i]) for i in range(len(loc))]

        self.weights_ = weights_
        self.n_dim_ = ndim
        #self.means_ = X
        #self.covariances_ = pw_variances
        return self


class JaccardPointwiseGaussianMixture(DensityMixin, BaseEstimator):

    def __init__(
        self,
        pointwse_distribution = 'normal',
        alpha = 1,
        scale_variance_pointwise = True,
        variance_mapper = 'log',
        min_var_factor = 1e-2,
    ):

        self.pointwse_distribution = pontiwse_distribution
        self.alpha = alpha
        self.scale_variance_pointwise = scale_variance_pointwise
        self.variance_mapper = variance_mapper
        self.min_var_factor = min_var_factor
        return

    def fit(self, X, jac_dists = None, **kwargs):
        '''
        fits a mixture estimating pointwise varainces from proposed method
        '''

        X = check_array(
            X,
            force_all_finite = True,
            ensure_2d = True
        )

        if X.shape[-1] == 1:
            X = X.flatten()
            ndim = 1
        else:
            ndim = X.shape[-1]

        #estimate global mean and cov
        pw_variances = pointwise_variance(
            values = X,
            jac_dists = jac_dists,
            min_var_factor = self.min_var_factor,
            alpha = self.alpha,
            variance_mapper = self.variance_mapper
        )

        weights_ = (1 - jac_dists)**self.alpha
        weights_ = weights_/weights_.sum()

        if ndim == 1:
            pointwise_std = np.sqrt(pw_variances)
            self.dists_ = [partial(np.random.normal, loc = X[i], scale = pointwise_std[i]) for i in range(len(X))]
        else:
            self.dists_ = [partial(np.random.multivariate_normal, mean = X[i], cov = pw_variances[i]) for i in range(len(X))]

        self.weights_ = weights_
        self.n_dim_ = ndim
        #self.means_ = X
        #self.covariances_ = pw_variances
        return self

    def sample(self, size = 1):
        '''
        draw samples from mixture
        '''
        idxs = np.random.choice(np.arange(len(self.dists_)), size = size, replace = True, p = self.weights_)
        counts = np.bincount(idxs)
        msk = counts > 0
        unique, counts = np.arange(len(counts))[msk], counts[msk]

        samples = []
        for i in range(len(unique)):
            idx = unique[i]
            n_samples = counts[i]
            sampled_values = self.dists_[idx](size = n_samples)
            samples.append(sampled_values)

        if sampled_values.ndim > 1:
            samples = np.vstack(samples)
        else:
            samples = np.vstack([s.reshape(-1,1) for s in samples])

        return samples