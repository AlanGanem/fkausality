# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/dist.ipynb (unless otherwise specified).

__all__ = ['get_distribution_var_factor_jaccard', 'pointwise_variance', 'sample_from_neighbors_continuous',
           'get_distribution_var_factor_jaccard', 'pointwise_variance', 'sample_from_neighbors_continuous']

# Cell
import numpy as np
from scipy import stats

# Cell
def get_distribution_var_factor_jaccard(jac_dists, min_var_factor = 1e-2, alpha = 1, func = 'log'):
    '''
    gets the variance factor of the "point distribution" given a jaccard distance from
    the query point

    to get the actual variance, use the variance factor alongside the variance of the
    observed variable in the neighbor points.

    example:

    get the 30 nearest neighbors, calculat their variance along some axis,
    then calculate the variance factor of each point and then multiply to get the
    "point variance in the contribution" for each point

    functions to test:
    tangent(x), 1/x, min_var - log(1-x)
    '''
    funcs = ['log', 'inverse_dist']
    assert min_var_factor > 0
    assert alpha > 0

    #if sim = 1, var_factor = min_var_factor, if sim -> 0, var_factor -> inf
    if func == 'log':
        var_factor = min_var_factor - alpha*np.log(1-jac_dists)
    elif func == 'inverse_dist':
        var_factor = min_var_factor/(1-jac_dists)**alpha
    return var_factor

def pointwise_variance(values, jac_dists, min_var_factor = 1e-2, alpha = 1, variance_mapper = 'log'):
    '''
    gets the pointwise variance for each neighbor of a given point, considering
    the variance of the neighborhood and the variance factor of each point

    each point contributes to the estimation of the queried point, but this contribution
    has a variance associated with how much this point is alike the queried point. in this sense
    pointwise variance is defined, as the variance of the contribution of each point, given the
    variance of all the points and how similar the points in the neighborhood are to the queried point

    the variance of the contribution os different from the (sampling)weight of the point,
    since points with lower similarity will have a higher variance, but will be less likely to be sampled
    '''
    if len(values.shape) > 1:
        if values.shape[-1] > 1:
            raise ValueError(f'value should have only one dimension along last axis, got shape {values.shape}')
        else:
            values = values.flatten()

    #global variance is calculated with the wieghted full neighbors instead of the sampled values, in order to avoid 0 variance
    #if a single value is sampled
    var = np.cov(values, aweights=(1- jac_dists)**alpha) #global variance weights are proportional to point similarities, not distance
    var_factor = get_distribution_var_factor_jaccard(jac_dists, min_var_factor, alpha, func = variance_mapper)
    pointwise_variance = var*var_factor
    return pointwise_variance

# Cell

def sample_from_neighbors_continuous(
    neighborhood_values,
    jac_dists,
    size = 100,
    noise_type = 'normal',
    alpha = 1,
    variance_mapper = 'log',
    min_var_factor = 1e-2,
    var_preprocess = np.sqrt
):
    '''
    samples from neighbor points with noise

    returns (samples, idxs)
    '''
    if len(neighborhood_values.shape) <= 1:
        neighborhood_values = neighborhood_values.reshape(-1,1)

    pw_var = pointwise_variance(neighborhood_values, jac_dists = jac_dists, min_var_factor = min_var_factor, alpha = alpha)
    #apply alppha and l1-normalize sample weights
    sample_weights = (1- jac_dists)**alpha #transform distance into similarity with 1 - jac_d
    sample_weights = sample_weights/sample_weights.sum()
    #sample based on sample weights
    sampled_idxs = np.random.choice(np.arange(neighborhood_values.shape[0]),size = size, p = sample_weights, replace = True)
    sampled_idxs = sorted(sampled_idxs)
    unique, counts = np.unique(sampled_idxs, return_counts=True)

    noise_type = getattr(np.random, noise_type)

    samples = []
    for i in range(unique.shape[0]):
        var = pw_var[unique[i]]

        noise = noise_type(0, var_preprocess(var), size = (counts[i],neighborhood_values.shape[-1]))
        sampled_values = neighborhood_values[unique[i]] + noise
        sample_idx = sampled_idxs[unique[i]]
        #samples = np.vstack([samples, sampled_values])
        samples.append(sampled_values)

    samples = np.vstack(samples)

    return samples, sampled_idxs


# Cell
import numpy as np
from scipy import stats

# Cell
def get_distribution_var_factor_jaccard(jac_dists, min_var_factor = 1e-2, alpha = 1, func = 'log'):
    '''
    gets the variance factor of the "point distribution" given a jaccard distance from
    the query point

    to get the actual variance, use the variance factor alongside the variance of the
    observed variable in the neighbor points.

    example:

    get the 30 nearest neighbors, calculat their variance along some axis,
    then calculate the variance factor of each point and then multiply to get the
    "point variance in the contribution" for each point

    functions to test:
    tangent(x), 1/x, min_var - log(1-x)
    '''
    funcs = ['log', 'inverse_dist']
    assert min_var_factor > 0
    assert alpha > 0

    #if sim = 1, var_factor = min_var_factor, if sim -> 0, var_factor -> inf
    if func == 'log':
        var_factor = min_var_factor - alpha*np.log(1-jac_dists)
    elif func == 'inverse_dist':
        var_factor = min_var_factor/(1-jac_dists)**alpha
    return var_factor

def pointwise_variance(values, jac_dists, min_var_factor = 1e-2, alpha = 1, variance_mapper = 'log'):
    '''
    gets the pointwise variance for each neighbor of a given point, considering
    the variance of the neighborhood and the variance factor of each point

    each point contributes to the estimation of the queried point, but this contribution
    has a variance associated with how much this point is alike the queried point. in this sense
    pointwise variance is defined, as the variance of the contribution of each point, given the
    variance of all the points and how similar the points in the neighborhood are to the queried point

    the variance of the contribution os different from the (sampling)weight of the point,
    since points with lower similarity will have a higher variance, but will be less likely to be sampled
    '''
    if len(values.shape) > 1:
        if values.shape[-1] > 1:
            raise ValueError(f'value should have only one dimension along last axis, got shape {values.shape}')
        else:
            values = values.flatten()

    #global variance is calculated with the wieghted full neighbors instead of the sampled values, in order to avoid 0 variance
    #if a single value is sampled
    var = np.cov(values, aweights=(1- jac_dists)**alpha) #global variance weights are proportional to point similarities, not distance
    var_factor = get_distribution_var_factor_jaccard(jac_dists, min_var_factor, alpha, func = variance_mapper)
    pointwise_variance = var*var_factor
    return pointwise_variance

# Cell

def sample_from_neighbors_continuous(
    neighborhood_values,
    jac_dists,
    size = 100,
    noise_type = 'normal',
    alpha = 1,
    variance_mapper = 'log',
    min_var_factor = 1e-2,
    var_preprocess = np.sqrt
):
    '''
    samples from neighbor points with noise

    returns (samples, idxs)
    '''
    if len(neighborhood_values.shape) <= 1:
        neighborhood_values = neighborhood_values.reshape(-1,1)

    pw_var = pointwise_variance(neighborhood_values, jac_dists = jac_dists, min_var_factor = min_var_factor, alpha = alpha)
    #apply alppha and l1-normalize sample weights
    sample_weights = (1- jac_dists)**alpha #transform distance into similarity with 1 - jac_d
    sample_weights = sample_weights/sample_weights.sum()
    #sample based on sample weights
    sampled_idxs = np.random.choice(np.arange(neighborhood_values.shape[0]),size = size, p = sample_weights, replace = True)
    sampled_idxs = sorted(sampled_idxs)
    unique, counts = np.unique(sampled_idxs, return_counts=True)

    noise_type = getattr(np.random, noise_type)

    samples = []
    for i in range(unique.shape[0]):
        var = pw_var[unique[i]]

        noise = noise_type(0, var_preprocess(var), size = (counts[i],neighborhood_values.shape[-1]))
        sampled_values = neighborhood_values[unique[i]] + noise
        sample_idx = sampled_idxs[unique[i]]
        #samples = np.vstack([samples, sampled_values])
        samples.append(sampled_values)

    samples = np.vstack(samples)

    return samples, sampled_idxs
